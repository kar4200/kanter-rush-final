perc_unemployed, perc_children_in_poverty,
perc_single_parent_households,
association_rate, violent_crime_rate,
injury_death_rate, household_income,
perc_free_or_reduced_lunch, perc_homeowners,
household_has_broadband,
household_has_computer, household_has_smartphone,
housing_mobile_homes, per_capita_income,
persons_per_household, income_ratio,
perc_with_access, perc_limited_access, food_environment_index)
p7 = ggcorrplot(cor(social_economic_environment),
colors = c("#305029", "#FFFFFF", "#8d1919"),
hc.order = FALSE,
type = "lower",
lab = TRUE,
lab_size = 1.5,
tl.cex = 6.5,
ggtheme = ggplot2::theme_bw())
ggsave(filename = "results/corr-social-economic-environment.png",
plot = p7,
device = "png",
width = 6,
height = 6)
physical_environment = mental_health_train %>%
select(average_daily_pm2.5, presence_of_violation, perc_severe_housing_cost_burden,
perc_severe_housing_problems,
overcrowding, inadequate_facilities, perc_drive_alone,
perc_long_commute_drives_alone, perc_rural)
p8 = ggcorrplot(cor(physical_environment),
colors = c("#305029", "#FFFFFF", "#8d1919"),
hc.order = TRUE,
type = "lower",
lab = TRUE,
lab_size = 2,
tl.cex = 6.5,
outline.color = "gray",
ggtheme = ggplot2::theme_bw())
ggsave(filename = "results/corr-physical-environment.png",
plot = p8,
device = "png",
width = 5,
height = 5)
# load libraries
library(tidyverse)
# read in the training data
mental_health_train = read_csv("data/clean/mental_health_train.csv")
# running a logistic regression
lm_fit = lm(mentally_unhealthy_days ~ . -physically_unhealthy_days,
data = mental_health_train)
# read in the testing data
mental_health_test = read_csv("data/clean/mental_health_test.csv")
# extracting the fitted probabilities
predictions_test_lm = predict(lm_fit,
newdata = mental_health_test)
predictions_train_lm = predict(lm_fit,
newdata = mental_health_train)
# evaluating the classifier - calculate rmse for train and test
mse_test_lm = mean((predictions_test_lm -
mental_health_test$mentally_unhealthy_days)^2) %>%
as_tibble()
mse_train_lm = mean((predictions_train_lm -
mental_health_train$mentally_unhealthy_days)^2) %>%
as_tibble()
mse_test_lm
mse_train_lm
save(lm_fit, file = "results/lm_fit.Rda")
# OLS regression
load("results/lm_fit.Rda")
# evaluate OLS mse
# test
predictions_test_lm = predict(lm_fit, newdata = mental_health_test)
mse_test_lm = mean((predictions_test_lm - mental_health_test$mentally_unhealthy_days)^2)
# train
predictions_train_lm = predict(lm_fit, newdata = mental_health_train)
mse_train_lm = mean((predictions_train_lm - mental_health_train$mentally_unhealthy_days)^2)
mse_test_lm
mse_train_lm
# load libraries
library(glmnetUtils)
source("code/functions/plot_glmnet.R")
library(tidyverse)
library(kableExtra)
# read in the training data
mental_health_train = read_csv("data/clean/mental_health_train.csv")
# read in the test
mental_health_test = read_csv("data/clean/mental_health_test.csv")
# run lasso regression
set.seed(1)
lasso_fit = cv.glmnet(mentally_unhealthy_days ~. -physically_unhealthy_days,
alpha = 1,
nfolds = 10,
data = mental_health_train)
plot(lasso_fit)
plot_glmnet(lasso_fit, mental_health_train, features_to_plot = 10, lambda = lasso_fit$lambda.1se)
# save the lasso fit object
save(lasso_fit, file = "results/lasso_fit.Rda")
# visualize the fitted coefficients as a function of lambda
predictions_test_lasso = predict(lasso_fit,
newdata = mental_health_test,
s = "lambda.1se") %>%
as.numeric()
predictions_train_lasso = predict(lasso_fit,
newdata = mental_health_train,
s = "lambda.1se") %>%
as.numeric()
# rmse
mse_test_lasso = mean((predictions_test_lasso - mental_health_test$mentally_unhealthy_days)^2) %>%
as_tibble()
mse_train_lasso = mean((predictions_train_lasso - mental_health_train$mentally_unhealthy_days)^2) %>%
as_tibble()
mse_test_lasso
mse_train_lasso
# load lasso fit object
load("results/lasso_fit.Rda")
# evaluate lasso mse
# test
predictions_test_lasso = predict(lasso_fit, newdata = mental_health_test,
s = "lambda.1se") %>% as.numeric()
mse_test_lasso = mean((predictions_test_lasso - mental_health_test$mentally_unhealthy_days)^2)
mse_test_lasso
# train
predictions_train_lasso = predict(lasso_fit, newdata = mental_health_train,
s = "lambda.1se") %>% as.numeric()
mse_train_lasso = mean((predictions_train_lasso - mental_health_train$mentally_unhealthy_days)^2)
mse_train_lasso
lasso_fit$nzero[lasso_fit$lambda == lasso_fit$lambda.1se]
# load libraries
library(glmnetUtils)
source("code/functions/plot_glmnet.R")
library(tidyverse)
# read in the training data
mental_health_train = read_csv("data/clean/mental_health_train.csv")
# read in the test
mental_health_test = read_csv("data/clean/mental_health_test.csv")
# run ridge regression
set.seed(1)
ridge_fit = cv.glmnet(mentally_unhealthy_days ~ .- physically_unhealthy_days,
alpha = 0,
nfolds = 10,
data = mental_health_train)
plot(ridge_fit)
plot_glmnet(ridge_fit, mental_health_train, features_to_plot = 10, lambda = ridge_fit$lambda.1se)
coef(ridge_fit, s = "lambda.1se")
# visualize the fitted coefficients as a function of lambda
predictions_test_ridge = predict(ridge_fit,
newdata = mental_health_test,
s = "lambda.1se") %>%
as.numeric()
predictions_train_ridge = predict(ridge_fit,
newdata = mental_health_train,
s = "lambda.1se") %>%
as.numeric()
# evaluating the classifier
mse_test_ridge = mean((predictions_test_ridge -
mental_health_test$mentally_unhealthy_days)^2) %>%
as_tibble()
mse_train_ridge = mean((predictions_train_ridge - mental_health_train$mentally_unhealthy_days)^2) %>%
as_tibble()
mse_test_ridge
mse_train_ridge
# save the ridge fit object
save(ridge_fit, file = "results/ridge_fit.Rda")
# load ridge fit object
load("results/ridge_fit.Rda")
# evaluate ridge mse
# test
predictions_test_ridge = predict(ridge_fit, newdata = mental_health_test,
s = "lambda.1se") %>% as.numeric()
mse_test_ridge = mean((predictions_test_ridge - mental_health_test$mentally_unhealthy_days)^2)
mse_test_ridge
# train
predictions_train_ridge = predict(ridge_fit, newdata = mental_health_train,
s = "lambda.1se") %>% as.numeric()
mse_train_ridge = mean((predictions_train_ridge - mental_health_train$mentally_unhealthy_days)^2)
mse_train_ridge
# optimal lambda value
lambda = ridge_fit$lambda.1se
# optimal lambda value
ridge_fit$lambda.1se
# load libraries
library(rpart)         # to train decision trees
library(rpart.plot)    # to plot decision trees
library(randomForest)  # random forests
library(gbm)           # boosting
library(tidyverse)     # tidyverse
library(kableExtra)    # kable
# read in the training data
mental_health_train = read_csv("data/clean/mental_health_train.csv")
# read in the testing data
mental_health_test = read_csv("data/clean/mental_health_test.csv")
# fit decision tree
set.seed(1)
mental_health_fit = rpart(mentally_unhealthy_days ~ . -physically_unhealthy_days,
data = mental_health_train)
rpart.plot(mental_health_fit)
# find deepest possible tree (to use for pruning to find optimal tree)
set.seed(1)
mental_health_fit_deep = rpart(mentally_unhealthy_days ~ . -physically_unhealthy_days,
control = rpart.control(minsplit = 2,
minbucket = 1,
cp = 0),
data = mental_health_train)
printcp(mental_health_fit_deep)
cp_table = printcp(mental_health_fit_deep) %>%
as_tibble()
cp = cp_table %>%
filter(nsplit >= 2) %>%
ggplot(aes(x = nsplit+1, y = xerror,
ymin = xerror - xstd, ymax = xerror + xstd)) +
geom_point() +
geom_line() +
scale_x_log10() +
geom_errorbar(width = 0.1) +
xlab("Number of terminal nodes") + ylab("CV error") +
geom_hline(aes(yintercept = min(xerror)), linetype = "dashed") +
theme_bw()
cp
# find optimal tree
set.seed(1)
optimal_tree_info = cp_table %>%
filter(xerror - xstd < min(xerror)) %>%
arrange(nsplit) %>%
head(1)
optimal_tree_info$nsplit # 17 splits in the optimal tree - has node with 0 (weird!)
# prune the optimal tree
optimal_tree_info
optimal_tree = prune(mental_health_fit_deep, cp = optimal_tree_info$CP)
rpart.plot(optimal_tree)
# mean-square-error
pred_decision_test = predict(optimal_tree,
newdata = mental_health_test)
pred_decision_train = predict(optimal_tree,
newdata = mental_health_train)
mse_decision_test = mean((pred_decision_test - mental_health_test$mentally_unhealthy_days)^2) %>%
as_tibble()
mse_decision_train = mean((pred_decision_train - mental_health_train$mentally_unhealthy_days)^2) %>%
as_tibble()
mse_decision_test
mse_decision_train
save(optimal_tree, file = "results/optimal_tree.Rda")
# load decision tree (optimal) object
load("results/optimal_tree.Rda")
# evaluate decision tree mse
# test
pred_decision_test = predict(optimal_tree,
newdata = mental_health_test)
mse_decision_test = mean((pred_decision_test - mental_health_test$mentally_unhealthy_days)^2)
mse_decision_test
# train
pred_decision_train = predict(optimal_tree,
newdata = mental_health_train)
mse_decision_train = mean((pred_decision_train - mental_health_train$mentally_unhealthy_days)^2)
mse_decision_train
# Random Forests
set.seed(1)
rf_fit = randomForest(mentally_unhealthy_days ~ . -physically_unhealthy_days,
data = mental_health_train)
rf_fit$mtry
# tune random forest
set.seed(1)
rf_fit_tuned = randomForest(mentally_unhealthy_days ~ . -physically_unhealthy_days,
mtry = 31,
ntree = 500,
importance = TRUE,
data = mental_health_train)
plot(rf_fit_tuned)
varImpPlot(rf_fit_tuned, n.var = 10, cex = 0.8,
main = "Random Forest Importance")
# mean-squared error
pred_rf_test = predict(rf_fit_tuned,
newdata = mental_health_test)
pred_rf_train = predict(rf_fit_tuned,
newdata = mental_health_train)
mse_rf_test = mean((pred_rf_test - mental_health_test$mentally_unhealthy_days)^2) %>%
as_tibble()
mse_rf_train = mean((pred_rf_train - mental_health_train$mentally_unhealthy_days)^2) %>%
as_tibble()
mse_rf_test
mse_rf_train
save(rf_fit_tuned, file = "results/rf_fit_tuned.Rda")
# load random forest object
load("results/rf_fit_tuned.Rda")
# evaluate random forest mse
# test
pred_rf_test = predict(rf_fit_tuned,
newdata = mental_health_test)
mse_rf_test = mean((pred_rf_test - mental_health_test$mentally_unhealthy_days)^2)
mse_rf_test
# train
pred_rf_train = predict(rf_fit_tuned,
newdata = mental_health_train)
mse_rf_train = mean((pred_rf_train - mental_health_train$mentally_unhealthy_days)^2)
mse_rf_train
# load libraries
library(rpart)         # to train decision trees
library(rpart.plot)    # to plot decision trees
library(randomForest)  # random forests
library(gbm)           # boosting
library(tidyverse)     # tidyverse
library(kableExtra)    # kable
# read in the training data
mental_health_train = read_csv("data/clean/mental_health_train.csv")
# read in the testing data
mental_health_test = read_csv("data/clean/mental_health_test.csv")
# fit boosted model
set.seed(1)
gbm_fit_1 = gbm(mentally_unhealthy_days ~ . -physically_unhealthy_days,
distribution = "gaussian",
n.trees = 1000,
interaction.depth = 1,
shrinkage = 0.1,
cv.folds = 5,
data = mental_health_train)
set.seed(1)
gbm_fit_2 = gbm(mentally_unhealthy_days ~ . -physically_unhealthy_days,
distribution = "gaussian",
n.trees = 1000,
interaction.depth = 2,
shrinkage = 0.1,
cv.folds = 5,
data = mental_health_train)
set.seed(1)
gbm_fit_3 = gbm(mentally_unhealthy_days ~ . -physically_unhealthy_days,
distribution = "gaussian",
n.trees = 1000,
interaction.depth = 3,
shrinkage = 0.1,
cv.folds = 5,
data = mental_health_train)
ntrees = 1000
cv_errors = bind_rows(
tibble(ntree = 1:ntrees, cv_err = gbm_fit_1$cv.error, depth = 1),
tibble(ntree = 1:ntrees, cv_err = gbm_fit_2$cv.error, depth = 2),
tibble(ntree = 1:ntrees, cv_err = gbm_fit_3$cv.error, depth = 3),
)
cv_boosting = cv_errors %>%
ggplot(aes(x = ntree, y = cv_err, colour = factor(depth))) +
# add horizontal dashed lines at the minima of the three curves
geom_hline(yintercept = min(gbm_fit_1$cv.error),
linetype = "dashed", color = "red") +
geom_hline(yintercept = min(gbm_fit_2$cv.error),
linetype = "dashed", color = "green") +
geom_hline(yintercept = min(gbm_fit_3$cv.error),
linetype = "dashed", color = "blue") +
geom_line() +
# set colors to match horizontal line minima
scale_color_manual(labels = c("1", "2", "3"),
values = c("red", "green", "blue")) +
labs(x = "Number of trees", y = "CV error", colour = "Interaction depth") +
theme_bw()
cv_boosting
# Extracting Optimal Tree
gbm_fit_tuned = gbm_fit_3
optimal_num_trees = gbm.perf(gbm_fit_3, plot.it = FALSE)
optimal_num_trees
summary(gbm_fit_tuned, n.trees = optimal_num_trees, plotit = FALSE) %>%
remove_rownames() %>%
top_n(10)
# mean squared error - test
predictions_test_gbm = predict(gbm_fit_tuned,
n.trees = optimal_num_trees,
newdata = mental_health_test)
mse_gbm_test = mean((predictions_test_gbm -
mental_health_test$mentally_unhealthy_days)^2) %>%
as_tibble()
mse_gbm_test
# mean squared error - train
predictions_train_gbm = predict(gbm_fit_tuned,
n.trees = optimal_num_trees,
newdata = mental_health_train)
mse_gbm_train = mean((predictions_train_gbm -
mental_health_train$mentally_unhealthy_days)^2) %>%
as_tibble()
mse_gbm_train
save(gbm_fit_tuned, file = "results/gbm_fit_tuned.Rda")
# load boosting object
load("results/gbm_fit_tuned.Rda")
# train
predictions_train_gbm = predict(gbm_fit_tuned,
n.trees = optimal_num_trees,
newdata = mental_health_train)
mse_gbm_train = mean((predictions_train_gbm -
mental_health_train$mentally_unhealthy_days)^2)
mse_gbm_train
# evaluate boosting mse
# test
predictions_test_gbm = predict(gbm_fit_tuned,
n.trees = optimal_num_trees,
newdata = mental_health_test)
mse_gbm_test = mean((predictions_test_gbm -
mental_health_test$mentally_unhealthy_days)^2)
mse_gbm_test
setwd("~/Desktop/stat-471-fall-2021/kanter-rush-final/results")
# load libraries
library(glmnetUtils)
library(tidyverse)
# read in the training data
mental_health_train = read_csv("data/clean/mental_health_train.csv")
setwd("~/Desktop/stat-471-fall-2021/kanter-rush-final")
# read in the training data
mental_health_train = read_csv("data/clean/mental_health_train.csv")
# read in the testing data
mental_health_test = read_csv("data/clean/mental_health_test.csv")
# OLS regression
load("results/lm_fit.Rda")
# load ridge fit object
load("results/ridge_fit.Rda")
# load lasso fit object
load("results/lasso_fit.Rda")
# load decision tree (optimal) object
load("results/optimal_tree.Rda")
# load random forest object
load("results/rf_fit_tuned.Rda")
# load boosting object
load("results/gbm_fit_tuned.Rda")
# evaluate OLS mse
# test
predictions_test_lm = predict(lm_fit, newdata = mental_health_test)
mse_test_lm = mean((predictions_test_lm - mental_health_test$mentally_unhealthy_days)^2)
# train
predictions_train_lm = predict(lm_fit, newdata = mental_health_train)
mse_train_lm = mean((predictions_train_lm - mental_health_train$mentally_unhealthy_days)^2)
# evaluate ridge mse
# test
predictions_test_ridge = predict(ridge_fit, newdata = mental_health_test,
s = "lambda.1se") %>% as.numeric()
mse_test_ridge = mean((predictions_test_ridge - mental_health_test$mentally_unhealthy_days)^2)
# train
predictions_train_ridge = predict(ridge_fit, newdata = mental_health_train,
s = "lambda.1se") %>% as.numeric()
mse_train_ridge = mean((predictions_train_ridge - mental_health_train$mentally_unhealthy_days)^2)
# evaluate lasso mse
# test
predictions_test_lasso = predict(lasso_fit, newdata = mental_health_test,
s = "lambda.1se") %>% as.numeric()
mse_test_lasso = mean((predictions_test_lasso - mental_health_test$mentally_unhealthy_days)^2)
# train
predictions_train_lasso = predict(lasso_fit, newdata = mental_health_train,
s = "lambda.1se") %>% as.numeric()
mse_train_lasso = mean((predictions_train_lasso - mental_health_train$mentally_unhealthy_days)^2)
# evaluate decision tree mse
# test
pred_decision_test = predict(optimal_tree,
newdata = mental_health_test)
mse_decision_test = mean((pred_decision_test - mental_health_test$mentally_unhealthy_days)^2)
# train
pred_decision_train = predict(optimal_tree,
newdata = mental_health_train)
mse_decision_train = mean((pred_decision_train - mental_health_train$mentally_unhealthy_days)^2)
# evaluate random forest mse
# test
pred_rf_test = predict(rf_fit_tuned,
newdata = mental_health_test)
mse_rf_test = mean((pred_rf_test - mental_health_test$mentally_unhealthy_days)^2)
# train
pred_rf_train = predict(rf_fit_tuned,
newdata = mental_health_train)
mse_rf_train = mean((pred_rf_train - mental_health_train$mentally_unhealthy_days)^2)
# evaluate boosting mse
# test
predictions_test_gbm = predict(gbm_fit_tuned,
n.trees = optimal_num_trees,
newdata = mental_health_test)
mse_gbm_test = mean((predictions_test_gbm -
mental_health_test$mentally_unhealthy_days)^2)
# train
predictions_train_gbm = predict(gbm_fit_tuned,
n.trees = optimal_num_trees,
newdata = mental_health_train)
mse_gbm_train = mean((predictions_train_gbm -
mental_health_train$mentally_unhealthy_days)^2)
setwd("~/Desktop/stat-471-fall-2021/kanter-rush-final/results")
# print nice table
tibble(Method = c("OLS", "Ridge", "Lasso", "Decision Tree", "Random Forest", "Boosting"),
`Train mse` = c(mse_train_lm, mse_train_ridge, mse_train_lasso, mse_decision_train ,mse_rf_train, mse_gbm_train),
`Test mse` = c(mse_test_lm, mse_test_ridge, mse_test_lasso, mse_decision_test ,mse_rf_test, mse_gbm_test)) %>%
kable(format = "latex",
booktabs = TRUE,
digits = 5,
col.names = c("Model", "Train MSE", "Test MSE")) %>%
save_kable("mse-all-models.pdf")
# load libraries
library(kableExtra)                     # for printing tables
library(cowplot)                        # for side by side plots
library(lubridate)                      # for dealing with dates
library(maps)                           # for creating maps
library(tidyverse)
# read in the cleaned data
mental_health_clean = read_csv("data/clean/mental_health_clean.csv")
setwd("~/Desktop/stat-471-fall-2021/kanter-rush-final")
# read in the cleaned data
mental_health_clean = read_csv("data/clean/mental_health_clean.csv")
# create histogram of mentally unhealthy days in dataset
mean <- mean(mental_health_clean$mentally_unhealthy_days) # save the mean
# plot mentally_unhealthy days and draw line at the mean
p = mental_health_clean %>%
ggplot(aes(x = mentally_unhealthy_days)) +
geom_histogram(fill = "light grey",
col = "black") +
geom_vline(xintercept = mean,
linetype = "dashed",
col = "red") +
labs(x = "Mentally Unhealthy Days (Per Month)",
y = "Number of Counties") +
theme_bw() +
ggtitle("Histogram of Mentally Unhealthy Days")
# save the histogram
ggsave(filename = "results/response-histogram.png",
plot = p,
device = "png",
width = 5,
height = 3)
