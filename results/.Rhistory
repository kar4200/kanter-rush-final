setwd("~/Desktop/stat-471-fall-2021/kanter-rush-final")
# load libraries
library(glmnetUtils)
source("code/functions/plot_glmnet.R")
library(tidyverse)
# read in the training data
mental_health_train = read_csv("data/clean/mental_health_train.csv")
# read in the test
mental_health_test = read_csv("data/clean/mental_health_test.csv")
# run lasso regression
set.seed(1)
lasso_fit = cv.glmnet(mentally_unhealthy ~ . -mentally_unhealthy_days
-physically_unhealthy_days,
alpha = 1,
nfolds = 10,
family = "binomial",
type.measure = "class",
data = mental_health_train)
plot(lasso_fit)
plot_glmnet(lasso_fit, mental_health_train, features_to_plot = 10, lambda = lasso_fit$lambda.1se)
lasso_fit$nzero[lasso_fit$lambda == lasso_fit$lambda.1se]
# save the lasso fit object
save(lasso_fit, file = "results/lasso_fit.Rda")
png(width = 7,
height = 5,
res = 300,
units = "in",
filename = "results/lasso-cv-plot.png")
plot(lasso_fit)
dev.off()
# create lasso trace plot
p_lasso = plot_glmnet(lasso_fit, mental_health_train, features_to_plot = 8)
ggsave(filename = "results/lasso-trace-plot.png",
plot = p_lasso,
device = "png",
width = 8,
height = 5)
# extract features selected by lasso and their coefficients
beta_hat_std = extract_std_coefs(lasso_fit, mental_health_train)
setwd("~/Desktop/stat-471-fall-2021/kanter-rush-final/results")
# reminder when saving kable: set working dictionary to "results"
beta_hat_std %>%
filter(coefficient != 0) %>%
arrange(desc(abs(coefficient))) %>%
head(10) %>%
kable(format = "latex",
booktabs = TRUE,
col.names = c("Feature", "Coefficient")) %>%
save_kable("lasso-coefficients.pdf")
library(kableExtra)
# reminder when saving kable: set working dictionary to "results"
beta_hat_std %>%
filter(coefficient != 0) %>%
arrange(desc(abs(coefficient))) %>%
head(10) %>%
kable(format = "latex",
booktabs = TRUE,
col.names = c("Feature", "Coefficient")) %>%
save_kable("lasso-coefficients.pdf")
setwd("~/Desktop/stat-471-fall-2021/kanter-rush-final")
# visualize the fitted coefficients as a function of lambda
probabilities = predict(lasso_fit,
newdata = mental_health_test,
s = "lambda.1se",
type = "response") %>%
as.numeric()
# make predictions
predictions = as.numeric(probabilities > 0.5)
# evaluating the classifier
mental_health_test = mental_health_test %>%
mutate(predicted_mental_health = predictions)
# calculate misclassification rate
misclassification_lasso = mental_health_test %>%
summarise(mean(mentally_unhealthy != predicted_mental_health))
misclassification_lasso
write_csv(misclassification_lasso, file = "results/misclassification_lasso.csv")
# load libraries
library(rpart)         # to train decision trees
library(rpart.plot)    # to plot decision trees
library(randomForest)  # random forests
library(gbm)           # boosting
library(tidyverse)     # tidyverse
library(kableExtra)    # kable
# read in the training data
mental_health_train = read_csv("data/clean/mental_health_train.csv")
# read in the testing data
mental_health_test = read_csv("data/clean/mental_health_test.csv")
# fit classification tree based on Gini Index with default "control" parameters
set.seed(1)
mental_health_fit = rpart(mentally_unhealthy ~ . -mentally_unhealthy_days
-physically_unhealthy_days,
method = "class",
parms = list(split = "gini"),
data = mental_health_train)
rpart.plot(mental_health_fit)
# find deepest possible tree (to begin to find optimal tree)
set.seed(1)
mental_health_fit_deep = rpart(mentally_unhealthy ~ . -mentally_unhealthy_days
-physically_unhealthy_days,
method = "class",
control = rpart.control(minsplit = 2,
minbucket = 1,
cp = 0),
parms = list(split = "gini"),
data = mental_health_train)
cp_table = printcp(mental_health_fit_deep) %>%
as_tibble()
cp = cp_table %>%
filter(nsplit >= 2) %>%
ggplot(aes(x = nsplit+1, y = xerror,
ymin = xerror - xstd, ymax = xerror + xstd)) +
geom_point() +
geom_line() +
scale_x_log10() +
geom_errorbar(width = 0.1) +
xlab("Number of terminal nodes") + ylab("CV error") +
geom_hline(aes(yintercept = min(xerror)), linetype = "dashed") +
theme_bw()
cp
ggsave(filename = "results/cp-cv-chart.png",
plot = cp,
device = "png",
width = 6,
height = 6)
# find optimal tree
set.seed(1)
optimal_tree_info = cp_table %>%
filter(xerror - xstd < min(xerror)) %>%
arrange(nsplit) %>%
head(1)
optimal_tree_info$nsplit # 7 splits in the optimal tree
# prune the optimal tree
optimal_tree = prune(mental_health_fit_deep, cp = optimal_tree_info$CP)
classification = rpart.plot(optimal_tree)
save(optimal_tree, file = "results/optimal_tree.Rda")
png(width = 8,
height = 8,
res = 300,
units = "in",
filename = "results/classification-tree.png")
rpart.plot(optimal_tree)
dev.off()
# misclassification
pred_decision = predict(optimal_tree,
newdata = mental_health_test, type = "class")
misclassification_decision = as_tibble(mean(pred_decision != mental_health_test$mentally_unhealthy)) # 9.07
misclassification_decision
write_csv(misclassification_decision, file = "results/misclassification_decision.csv")
# RANDOM FORESTS
set.seed(1)
rf_fit = randomForest(factor(mentally_unhealthy) ~ . -mentally_unhealthy_days -physically_unhealthy_days,
data = mental_health_train)
rf_fit$mtry
# tune random forests
set.seed(1)
mvalues = seq.int(1, 60, by = 5)
oob_errors = numeric(length(mvalues))
ntree = 100
for(idx in 1:length(mvalues)){
m = mvalues[idx]
rf_fit_test = randomForest(factor(mentally_unhealthy) ~ . -mentally_unhealthy_days -physically_unhealthy_days,
mtry = m, data = mental_health_train)
oob_errors[idx] = rf_fit_test$err.rate[,"OOB"][ntree]
}
rf_cv = tibble(m = mvalues, oob_err = oob_errors) %>%
ggplot(aes(x = m, y = oob_err)) +
geom_line() + geom_point() +
scale_x_continuous(breaks = mvalues) +
theme_bw()
rf_cv
rf_cv = tibble(m = mvalues, oob_err = oob_errors) %>%
ggplot(aes(x = m, y = oob_err)) +
geom_line() + geom_point() +
scale_x_continuous(breaks = mvalues) +
theme_bw()
png(width = 7,
height = 7,
res = 300,
units = "in",
filename = "results/rf-cv-plot.png")
plot(rf_cv)
dev.off()
# tune random forest
set.seed(1)
rf_fit_tuned = randomForest(factor(mentally_unhealthy) ~ . -mentally_unhealthy_days -physically_unhealthy_days,
mtry = 21,
ntree = 500,
importance = TRUE,
data = mental_health_train)
save(rf_fit_tuned, file = "results/rf_fit_tuned.Rda")
# variable importance
var_imp = varImpPlot(rf_fit_tuned, n.var = 10, cex = 0.5)
# misclassification error
pred_rf = predict(rf_fit_tuned, newdata = mental_health_test, type = "class")
misclassification_rf = as_tibble(mean(pred_rf != mental_health_test$mentally_unhealthy)) # 7.38
misclassification_rf
write_csv(misclassification_rf, file = "results/misclassification_rf.csv")
# reading in misclassification errors
misclassification_regression = read_csv("results/misclassification_regression.csv")
misclassification_ridge = read_csv("results/misclassification_ridge.csv")
misclassification_lasso = read_csv("results/misclassification_lasso.csv")
misclassification_decision = read_csv("results/misclassification_decision.csv")
misclassification_rf = read_csv("results/misclassification_rf.csv")
# renaming column names
misclassification_regression = misclassification_regression %>%
rename(value = `mean(mentally_unhealthy != predicted_mental_health)`)
misclassification_ridge = misclassification_ridge %>%
rename(value = `mean(mentally_unhealthy != predicted_mental_health)`)
misclassification_lasso = misclassification_lasso %>%
rename(value = `mean(mentally_unhealthy != predicted_mental_health)`)
# creating table of misclassification errors
Model = c("Regression", "Ridge", "Lasso", "Decision", "Random Forest")
as_tibble(cbind(Model, rbind(misclassification_regression,
misclassification_ridge,
misclassification_lasso,
misclassification_decision,
misclassification_rf)))
setwd("~/Desktop/stat-471-fall-2021/kanter-rush-final/results")
misclassification =
as_tibble(cbind(Model, rbind(misclassification_regression,
misclassification_ridge,
misclassification_lasso,
misclassification_decision,
misclassification_rf))) %>%
kable(format = "latex",
booktabs = TRUE,
digits = 4,
col.names = c("Feature", "Coefficient")) %>%
save_kable("misclassification-error.pdf")
